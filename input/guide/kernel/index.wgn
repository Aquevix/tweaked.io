Title: Tuning the GNU/Linux Kernel
menu: menu
----

The GNU/Linux Kernel
====================

Linux is the world-leading open-source kernel.

It is designed to peform well on a wide range of hardware.


File Handle Limits
------------------

When you're serving a lot of traffic it is usually the case that the traffic you're serving is coming from a large number of local files.

The kernel has built-in limits on the number of files that a process can open, and raising these limits, at a cost of some system memory, is usually a sane thing to attempt.

You can view the current limit on the number of open-files by running:

<pre class="code">
cat /proc/sys/fs/file-max
</pre>

The limit can be raised interactively by running, as `root`:

<pre class="code">
sysctl -w fs.file-max=100000
</pre>

If you wish that change to be made persistently you should append to the file `/etc/sysctl.conf` the line:

<pre class="code">
fs.file-max = 100000
</pre>

Then run the following command to make your change take effect:

<pre class="code">
sysctl -p
</pre>


Socket Tuning
-------------

For servers which are handling large numbers of concurent sessions, there are some TCP options that should probabaly be tweaked.

With a large number of clients comnunicating with your server it wouldn't be unusual to have a 20,000 open sockets or more.  To increase that range you append the following to the bottom of `/etc/sysctl.conf`:

<pre class="code">
   # Use the full range of ports.
   net.ipv4.ip_local_port_range = 1024 65535
</pre>

You can also increase the recycling time of sockets, avoiding large numbers of them staying in the `TIME_WAIT` status by adding these values to `/etc/sysctl.conf`:

<pre class="code">
   # Enables fast recycling of TIME_WAIT sockets.
   # (Use with caution according to the kernel documentation!)
   net.ipv4.tcp_tw_recycle = 1

   # Allow reuse of sockets in TIME_WAIT state for new connections
   # only when it is safe from the network stackâ€™s perspective.
   net.ipv4.tcp_tw_reuse = 1
</pre>

Finally one problem you'll find is that if a socket is listening and
busy a connection-backlog will pile up.  The kernel will keep pending
connections in a buffer before failing.  You can tweak several values
to increase the size of the backlog:

<pre class="code">
   #
   # 16MB per socket - which sounds like a lot, but will virtually never
   # consume that much.
   #
   net.core.rmem_max = 16777216
   net.core.wmem_max = 16777216

   # Increase the number of outstanding syn requests allowed.
   # c.f. The use of syncookies.
   net.ipv4.tcp_max_syn_backlog = 4096
   net.ipv4.tcp_syncookies = 1

   # The maximum number of "backlogged sockets".  Default is 128.
   net.core.somaxconn = 1024
</pre>

The trade-off here is that a connecting client will see a slow connection,
but this is almost certainly better than a `Connection Refused` error.

Once you've made those additions you can cause them to be loaded by running:

<pre class="code">
sysctl -p
</pre>

Finally if you've changed these limits you will need to restart the associated daemons.  (For example "`service nginx restart`".)


Filesystem Tuning
----------------

You almost certainly want to disable the "`atime`" option on your filesystems.

With this disabled that the last time a file was accessed won't be constantly updated every time you read a file, since this information isn't generally useful inand causes extra disk hits, its typically disabled.

To do this, just edit `/etc/fstab` and add "notime" as a mount option for the filesystem.  For example:

<pre class="code">
    /dev/rd/c0d0p3          /test                    ext3    noatime        1 2
</pre>


Swap Tuning
-----------

* **TODO**

RAID Tuning
-----------

It seems to be the case that if you have the `deadline` scheduler this is best for RAID setups, however this is something that you'll want to test yourself.

Boot your kernel with `elevator=deadline` appended to the command-line and compare the result via your favourite [filesystem test](/benchmarking/#fs).